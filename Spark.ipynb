{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, sum as spark_sum\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Convert Excel â†’ CSV first if not already done\n","csv_path = \"abfss://4818cd05-d34c-4929-abfe-b2870c622eb9@onelake.dfs.fabric.microsoft.com/2501c47d-b4c5-407d-965f-9d790532bf28/Files/sales.xlsx\"\n","\n","# Load CSV in Spark\n","df_spark = spark.read.csv(csv_path, header=True, inferSchema=True)\n","\n","# Add calculated column and group\n","df_spark = df_spark.withColumn(\"TotalSales\", col(\"Quantity\") * col(\"UnitPrice\"))\n","\n","result_spark = df_spark.groupBy(\"Item\").agg(spark_sum(\"TotalSales\").alias(\"TotalSales\"))\n","\n","result_spark.show(5)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"eafd6db4-1b09-4edc-af57-5fdd1a61b794","normalized_state":"finished","queued_time":"2025-09-19T09:31:18.4643368Z","session_start_time":null,"execution_start_time":"2025-09-19T09:31:18.4655688Z","execution_finish_time":"2025-09-19T09:31:23.3799282Z","parent_msg_id":"8f1e03c1-b115-43b9-b02c-79f12bad21e6"},"text/plain":"StatementMeta(, eafd6db4-1b09-4edc-af57-5fdd1a61b794, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+--------------------+------------------+\n|                Item|        TotalSales|\n+--------------------+------------------+\n|Mountain-200 Blac...| 844474.3533999989|\n|Touring-1000 Yell...| 176421.1800000003|\n|Touring-1000 Blue...|159732.69000000026|\n|Short-Sleeve Clas...|11661.839999999962|\n|Women's Mountain ...|10218.539999999975|\n+--------------------+------------------+\nonly showing top 5 rows\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad6eb82f-5601-4ca9-808b-68991562c3b9"},{"cell_type":"markdown","source":["1. Python (Pandas) Notebook Result\n","\n","Load Time: ~3.6 seconds (for reading Excel).\n","\n","GroupBy Time: ~0.04 seconds.\n","\n","Data Processed: All in memory, so very fast for a 1.6 MB file.\n","\n","Output: Direct aggregation (groupby Item â†’ TotalSales).\n","\n","ðŸ‘‰ Works great for small-to-medium datasets (up to a few million rows, depending on memory).\n","\n","ðŸ”¹ 2. Spark (PySpark) Notebook Result\n","\n","Execution Time: ~9.7 seconds (job distributed over Spark engine).\n","\n","Stages / Tasks: Multiple Spark jobs created â†’ data read, transformations, aggregations.\n","\n","Rows Processed: ~32K rows.\n","\n","Output: Same aggregation as Python but executed in distributed mode.\n","\n","ðŸ‘‰ Spark has overhead (job scheduling, task distribution), so for small files it is slower than Pandas. But for large datasets (GBs/TBs), Spark will scale while Pandas will run out of memory."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b941ccc9-6f2e-4a3a-a84d-94d6ebc9a856"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","import warnings\n","\n","try:\n","    spark = SparkSession.builder.getOrCreate()\n","except Exception as e:\n","    warnings.warn(f\"SparkSession already exists or failed to create: {e}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"140ae661-8608-4a4a-a8f6-475b387fc2e0","normalized_state":"finished","queued_time":"2025-09-09T09:35:27.2681947Z","session_start_time":null,"execution_start_time":"2025-09-09T09:35:40.1943153Z","execution_finish_time":"2025-09-09T09:35:40.5610759Z","parent_msg_id":"8f41c142-8e22-4bad-bfd9-c03777e8bf49"},"text/plain":"StatementMeta(, 140ae661-8608-4a4a-a8f6-475b387fc2e0, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"41d24efa-ea7b-44ca-8e77-75c0d04f5ca4"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, when, count, lit\n","import time, psutil, os\n","\n","# Initialize Spark session\n","spark = SparkSession.builder.master(\"local\").appName(\"DQ Validation\").getOrCreate()\n","\n","process = psutil.Process(os.getpid())\n","cpu_cores = psutil.cpu_count(logical=True)\n","\n","mem_before = process.memory_info().rss / (1024 ** 2)\n","cpu_start = process.cpu_times().user + process.cpu_times().system\n","start_time = time.time()\n","\n","# Sample DataFrame (replace with your actual Spark DataFrame)\n","data = [\n","    (\"col1\", \"A\", 1),\n","    (\"col2\", None, 2),\n","    (\"col3\", \"C\", 3),\n","    (\"col4\", \"D\", None),\n","    (\"col5\", None, 5)\n","]\n","columns = [\"Current Name\", \"Type\", \"Value\"]\n","\n","df_rename_config = spark.createDataFrame(data, columns)\n","\n","# Add New Column (equivalent to df['Type'] * 2 in Pandas)\n","df_rename_config = df_rename_config.withColumn(\"New Column\", col(\"Value\") * 2)\n","\n","# Fill Nulls in 'Current Name'\n","df_rename_config = df_rename_config.withColumn(\n","    \"Current Name\", when(col(\"Current Name\").isNull(), lit(\"Unknown\")).otherwise(col(\"Current Name\"))\n",")\n","\n","# Data Quality Report\n","dq_report = {}\n","\n","# Null Values per column\n","null_counts = df_rename_config.select([\n","    count(when(col(c).isNull(), c)).alias(c) for c in df_rename_config.columns\n","]).collect()[0].asDict()\n","\n","# Duplicate Rows\n","duplicate_rows = df_rename_config.count() - df_rename_config.dropDuplicates().count()\n","\n","dq_report[\"Null Values\"] = null_counts\n","dq_report[\"Duplicate Rows\"] = duplicate_rows\n","\n","end_time = time.time()\n","cpu_end = process.cpu_times().user + process.cpu_times().system\n","mem_after = process.memory_info().rss / (1024 ** 2)\n","\n","elapsed = end_time - start_time\n","cpu_used = cpu_end - cpu_start\n","cus_consumed = (cpu_used * cpu_cores) / 3600  \n","\n","print(\"Data After Manipulation (Spark):\")\n","df_rename_config.show()\n","\n","print(\"DQ Report (Spark):\", dq_report)\n","print(f\"Time Taken: {elapsed:.4f} sec\")\n","print(f\"CUs Consumed: {cus_consumed:.6f} approx\")\n","print(f\"CPU Time Used: {cpu_used:.4f} sec\")\n","print(f\"Memory Used: {mem_after - mem_before:.2f} MB\")\n","print(f\"Data Size: {df_rename_config.count()} rows Ã— {len(df_rename_config.columns)} cols\")\n","\n","spark.stop()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"eafd6db4-1b09-4edc-af57-5fdd1a61b794","normalized_state":"finished","queued_time":"2025-09-19T09:18:54.1698451Z","session_start_time":"2025-09-19T09:18:54.1710303Z","execution_start_time":"2025-09-19T09:19:07.1349642Z","execution_finish_time":"2025-09-19T09:19:15.0216292Z","parent_msg_id":"e1046a78-f554-4d2b-959a-f43855d47cb6"},"text/plain":"StatementMeta(, eafd6db4-1b09-4edc-af57-5fdd1a61b794, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Data After Manipulation (Spark):\n+------------+----+-----+----------+\n|Current Name|Type|Value|New Column|\n+------------+----+-----+----------+\n|        col1|   A|    1|         2|\n|        col2|NULL|    2|         4|\n|        col3|   C|    3|         6|\n|        col4|   D| NULL|      NULL|\n|        col5|NULL|    5|        10|\n+------------+----+-----+----------+\n\nDQ Report (Spark): {'Null Values': {'Current Name': 0, 'Type': 2, 'Value': 1, 'New Column': 1}, 'Duplicate Rows': 0}\nTime Taken: 4.5512 sec\nCUs Consumed: 0.000556 approx\nCPU Time Used: 0.2500 sec\nMemory Used: 4.13 MB\nData Size: 5 rows Ã— 4 cols\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2e88c282-76f9-4799-9f3a-fc76dda0d76d"},{"cell_type":"markdown","source":["| Metric                    | Python (Pandas)      | Spark (PySpark)      |\n","| ------------------------- | -------------------- | -------------------- |\n","| **Load Time**             | 2.86 sec             | 3.64 sec             |\n","| **Aggregation Time**      | 0.0035 sec           | 0.054 sec            |\n","| **Total Rows**            | 32,718               | 32,718               |\n","| **Memory Usage (approx)** | 331.29 MB (0.323 GB) | 250.09 MB (0.244 GB) |\n","| **CPU Usage (approx)**    | 3%                   | 38.9%                |\n","| **CUs Consumed**          | **983.1315**         | **9740.63**          |\n","| **Estimated Cost**        | **\\$0.0492/hour**    | **\\$0.4870/hour**    |\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"96de2d36-c90c-4321-9027-df337f206382"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1c6d4097-f3d5-48e0-83a3-70a1e6d9ccd5"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}